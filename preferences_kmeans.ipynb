{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('section_1.xlsx')\n",
    "combined_text = df['domains'].fillna('') + ' ' + df['projects'].fillna('')\n",
    "embeddings = model.encode(combined_text.tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1. Availability: parse into time-slot features\n",
    "def parse_availability(av_text):\n",
    "    slots = {slot: 0 for slot in ['weekday_morning','weekday_afternoon','weekday_evening',\n",
    "                                  'weekend_morning','weekend_afternoon','weekend_evening']}\n",
    "    if pd.isna(av_text):\n",
    "        return slots\n",
    "    text = av_text.lower()\n",
    "    if \"weekday\" in text:\n",
    "        if \"morning\" in text: slots['weekday_morning'] = 1\n",
    "        if \"afternoon\" in text: slots['weekday_afternoon'] = 1\n",
    "        if \"evening\" in text or \"night\" in text: slots['weekday_evening'] = 1\n",
    "        if \"all day\" in text or (\"morning\" not in text and \"afternoon\" not in text and \"evening\" not in text):\n",
    "            slots['weekday_morning'] = slots['weekday_afternoon'] = slots['weekday_evening'] = 1\n",
    "    if \"weekend\" in text:\n",
    "        if \"morning\" in text: slots['weekend_morning'] = 1\n",
    "        if \"afternoon\" in text: slots['weekend_afternoon'] = 1\n",
    "        if \"evening\" in text or \"night\" in text: slots['weekend_evening'] = 1\n",
    "        if \"all day\" in text or (\"morning\" not in text and \"afternoon\" not in text and \"evening\" not in text):\n",
    "            slots['weekend_morning'] = slots['weekend_afternoon'] = slots['weekend_evening'] = 1\n",
    "    return slots\n",
    "\n",
    "availability_features = df['availability'].apply(parse_availability).tolist()\n",
    "avail_df = pd.DataFrame(availability_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Work Distribution: One-hot encode the categories\n",
    "work_dist_ohe = pd.get_dummies(df['work_dist'].fillna('No preference'), prefix='work')\n",
    "\n",
    "\n",
    "# 4. Mentor attribute: binary encode\n",
    "mentor_flag = df['mentoring'].fillna('No').apply(lambda x: 1 if str(x).strip().lower() == 'yes' else 0)\n",
    "mentor_flag = mentor_flag.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avail_array = avail_df.values.astype(float)\n",
    "work_array = work_dist_ohe.values.astype(float)\n",
    "\n",
    "avail_weight = 3.0    \n",
    "interest_weight = 1.0  \n",
    "work_weight = 0.5    \n",
    "mentor_weight = 0.2 \n",
    "\n",
    "\n",
    "features = np.hstack([\n",
    "    avail_array * avail_weight,\n",
    "    embeddings * interest_weight,\n",
    "    work_array * work_weight,\n",
    "    mentor_flag * mentor_weight\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "max_group_size = 4\n",
    "initial_k = int(np.ceil(len(df) / max_group_size))  \n",
    "\n",
    "kmeans = KMeans(n_clusters=initial_k, random_state=42)\n",
    "labels = kmeans.fit_predict(features)\n",
    "\n",
    "cluster_dict = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    cluster_dict.setdefault(label, []).append(int(df.loc[idx, 'ID'])) \n",
    "initial_clusters = list(cluster_dict.values())\n",
    "print(f\"Initial clusters (count={len(initial_clusters)}):\", [len(c) for c in initial_clusters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def recluster_large_clusters(clusters, features, max_size=4):\n",
    "    \"\"\"Split clusters larger than max_size using KMeans (preserve original logic).\"\"\"\n",
    "    new_clusters = []\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) > max_size:\n",
    "            n_sub = (len(cluster) // max_size) + 1\n",
    "            sub_feats = [features[list(df['ID']).index(stu_id)] for stu_id in cluster]  # feature vectors of cluster members\n",
    "            sub_kmeans = KMeans(n_clusters=n_sub, random_state=42)\n",
    "            sub_labels = sub_kmeans.fit_predict(sub_feats)\n",
    "            sub_cluster_map = defaultdict(list)\n",
    "            for member, sub_label in zip(cluster, sub_labels):\n",
    "                sub_cluster_map[sub_label].append(member)\n",
    "\n",
    "            for sub_cluster in sub_cluster_map.values():\n",
    "                if len(sub_cluster) > max_size:\n",
    "                    new_clusters.extend(recluster_large_clusters([sub_cluster], features, max_size))\n",
    "                else:\n",
    "                    new_clusters.append(sub_cluster)\n",
    "        else:\n",
    "            new_clusters.append(cluster)\n",
    "    return new_clusters\n",
    "\n",
    "def combine_small_clusters(clusters, features, min_size=3, max_size=4):\n",
    "    \"\"\"Merge clusters smaller than min_size with nearest other clusters (by feature similarity) if possible.\"\"\"\n",
    "\n",
    "    small_clusters = [c for c in clusters if len(c) < min_size]\n",
    "    other_clusters = [c for c in clusters if len(c) >= min_size]\n",
    "    merged = False\n",
    "    for small in small_clusters:\n",
    "        if not small: \n",
    "            continue\n",
    "  \n",
    "        best_match = None\n",
    "        best_distance = float('inf')\n",
    "\n",
    "        small_indices = [list(df['ID']).index(stu_id) for stu_id in small]\n",
    "        small_centroid = np.mean(features[small_indices], axis=0)\n",
    "        for c in other_clusters:\n",
    "            if len(c) + len(small) <= max_size:\n",
    "                c_indices = [list(df['ID']).index(stu_id) for stu_id in c]\n",
    "                c_centroid = np.mean(features[c_indices], axis=0)\n",
    "                dist = np.linalg.norm(small_centroid - c_centroid)\n",
    "                if dist < best_distance:\n",
    "                    best_distance = dist\n",
    "                    best_match = c\n",
    "        if best_match is not None:\n",
    "            best_match.extend(small)\n",
    "            merged = True\n",
    "        else:\n",
    "            other_clusters.append(small)\n",
    "    if merged:\n",
    "        return combine_small_clusters(other_clusters, features, min_size, max_size)\n",
    "    else:\n",
    "        return other_clusters\n",
    "\n",
    "\n",
    "clusters = recluster_large_clusters(initial_clusters, features, max_size=4)\n",
    "clusters = combine_small_clusters(clusters, features, min_size=3, max_size=4)\n",
    "print(f\"Clusters after size balancing: {[len(c) for c in clusters]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anti_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    student_id = int(row['ID'])\n",
    "    if pd.notna(row['anti_pref']):\n",
    "        prefs = str(row['anti_pref']).split(',')\n",
    "        forbidden_ids = {int(p.strip()) for p in prefs if p.strip().isdigit()}\n",
    "        for forbidden_id in forbidden_ids:\n",
    "\n",
    "            anti_dict.setdefault(student_id, set()).add(forbidden_id)\n",
    "            anti_dict.setdefault(forbidden_id, set()).add(student_id)\n",
    "\n",
    "\n",
    "def enforce_antiprefs(clusters):\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for ci, cluster in enumerate(clusters):\n",
    "            violators = None\n",
    "            for sid in cluster:\n",
    "                if sid in anti_dict:\n",
    "                    conflict_ids = anti_dict[sid].intersection(cluster)\n",
    "                    if conflict_ids:\n",
    "                        violators = (sid, list(conflict_ids)[0])\n",
    "                        break\n",
    "            if violators:\n",
    "                sid_a, sid_b = violators\n",
    "                target_found = False\n",
    "                for cj, other_cluster in enumerate(clusters):\n",
    "                    if cj == ci:\n",
    "                        continue\n",
    "                    if len(other_cluster) < 4 and all(s not in anti_dict.get(sid_b, set()) for s in other_cluster):\n",
    "                        cluster.remove(sid_b)\n",
    "                        other_cluster.append(sid_b)\n",
    "                        changed = True\n",
    "                        target_found = True\n",
    "                        break\n",
    "                if not target_found:\n",
    "                    for cj, other_cluster in enumerate(clusters):\n",
    "                        if cj == ci: continue\n",
    "                        for swap_candidate in other_cluster:\n",
    "                            if swap_candidate not in anti_dict.get(sid_a, set()) and sid_b not in anti_dict.get(other_cluster[0], set()):\n",
    "                                cluster.remove(sid_b)\n",
    "                                other_cluster.remove(swap_candidate)\n",
    "                                cluster.append(swap_candidate)\n",
    "                                other_cluster.append(sid_b)\n",
    "                                changed = True\n",
    "                                target_found = True\n",
    "                                break\n",
    "                        if target_found: break\n",
    "                if changed:\n",
    "                    break\n",
    "    return clusters\n",
    "\n",
    "clusters = enforce_antiprefs(clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentors = {int(row['ID']) for i, row in df.iterrows() if str(row['mentoring']).strip().lower() == 'yes'}\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    mentor_ids = [sid for sid in cluster if sid in mentors]\n",
    "    if len(mentor_ids) > 1:\n",
    "        for sid in mentor_ids[1:]: \n",
    "            for j, other_cluster in enumerate(clusters):\n",
    "                if i == j: \n",
    "                    continue\n",
    "                if all(mid not in mentors for mid in other_cluster) and len(other_cluster) < 4:\n",
    "                    cluster.remove(sid)\n",
    "                    other_cluster.append(sid)\n",
    "                    break\n",
    "            if sid in cluster:  \n",
    "                for j, other_cluster in enumerate(clusters):\n",
    "                    if i == j: continue\n",
    "                    swap_idx = next((k for k, member in enumerate(other_cluster) if member not in mentors), None)\n",
    "                    if swap_idx is not None:\n",
    "                        swap_id = other_cluster[swap_idx]\n",
    "                        cluster.remove(sid)\n",
    "                        other_cluster[swap_idx] = sid\n",
    "                        cluster.append(swap_id)\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Clusters:\")\n",
    "for idx, cluster in enumerate(clusters, start=1):\n",
    "    print(f\"Group {idx} (size {len(cluster)}): {cluster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "\n",
    "for cluster_id, cluster in enumerate(clusters, start=1):\n",
    "    for student_id in cluster:\n",
    "        student_data = df[df[\"ID\"] == student_id].iloc[0].to_dict()\n",
    "        student_data[\"cluster_id\"] = cluster_id\n",
    "        final_data.append(student_data)\n",
    "\n",
    "df_final = pd.DataFrame(final_data)\n",
    "\n",
    "output_file = \"student_clusters.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Clustered data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

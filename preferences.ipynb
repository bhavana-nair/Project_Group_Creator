{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from hdbscan import HDBSCAN \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_base = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "all_mpnet = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# --------------------\n",
    "# Feature Engineering\n",
    "# --------------------\n",
    "def process_anti_preferences(pref_str, all_ids):\n",
    "    \"\"\"Process anti-preferences into exclusion list\"\"\"\n",
    "    if pd.isna(pref_str): return []\n",
    "    return [int(id) for id in re.findall(r'\\d+', str(pref_str)) if int(id) in all_ids]\n",
    "\n",
    "def create_anti_matrix(df):\n",
    "    \"\"\"Create anti-preference matrix with bidirectional conflicts.\"\"\"\n",
    "    n = len(df)\n",
    "    anti_matrix = np.zeros((n, n))\n",
    "    id_to_idx = {id: idx for idx, id in enumerate(df['ID'])}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        anti_ids = process_anti_preferences(row[7], df['ID'].values)\n",
    "        for a_id in anti_ids:\n",
    "            if a_id in id_to_idx:\n",
    "                anti_matrix[idx, id_to_idx[a_id]] = 1\n",
    "                anti_matrix[id_to_idx[a_id], idx] = 1  # Ensure bidirectional conflicts\n",
    "    return anti_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------\n",
    "# Weighted Embedding\n",
    "# ---------------------\n",
    "def get_weighted_embeddings(texts, weight,model):\n",
    "    \"\"\"Get weighted sentence embeddings\"\"\"\n",
    "    embeddings = model.encode(texts.fillna(''))\n",
    "    return embeddings * weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_students(file_path):\n",
    "    # Load and combine sections\n",
    "    df = pd.read_excel(file_path, sheet_name='Sec 1')\n",
    "    # df2 = pd.read_excel(file_path, sheet_name='Sec 2') \n",
    "    # df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "    # Feature Weights (adjust as needed)\n",
    "  weights = {\n",
    "    'anti_pref': 100.0,  # Very high weight to enforce anti-preferences\n",
    "    'availability': 5.0,  # High importance\n",
    "    'work_dist': 3.0,     # Medium importance\n",
    "    'domains': 1.0,       # Equal and lower importance\n",
    "    'projects': 1.0,      # Equal and lower importance\n",
    "    'mentoring': 0.5      # Lowest importance\n",
    "}\n",
    "\n",
    "    # Process each feature\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Availability\n",
    "    features['availability'] = get_weighted_embeddings(df['availability'], weights['availability'],all_mpnet)\n",
    "    \n",
    "    # 2. Work Distribution\n",
    "    features['work_dist'] =  get_weighted_embeddings(df['work_dist'], weights['work_dist'],all_mpnet)\n",
    "    \n",
    "    # 3. Mentoring\n",
    "    mentoring = df['mentoring'].notna().astype(int).values.reshape(-1, 1)\n",
    "    features['mentoring'] = mentoring * weights['mentoring']\n",
    "    \n",
    "    # 4. Anti-preferences\n",
    "    anti_matrix = create_anti_matrix(df)\n",
    "    features['anti_pref'] = anti_matrix * weights['anti_pref']\n",
    "    \n",
    "    # 5. Domains & Projects (LLM embeddings)\n",
    "    features['domains'] = get_weighted_embeddings(df['domains'], weights['domains'],bert_base)\n",
    "    features['projects'] = get_weighted_embeddings(df['projects'], weights['projects'],bert_base)\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.hstack(list(features.values()))\n",
    "    \n",
    "    # Clustering\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=3,\n",
    "        metric='cosine',\n",
    "        cluster_selection_method='leaf',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    # Create distance matrix with anti-preference penalties\n",
    "    distance_matrix = cosine_similarity(X)\n",
    "    distance_matrix += anti_matrix * 1000  # Large penalty for conflicts\n",
    "    \n",
    "    clusters = clusterer.fit_predict(distance_matrix)\n",
    "    \n",
    "    # Post-process clusters\n",
    "    result_df = pd.DataFrame({\n",
    "        'ID': df['ID'],\n",
    "        'Cluster': clusters,\n",
    "        'Domains': df.iloc[:, 1],\n",
    "        'Availability': df.iloc[:, 3],\n",
    "        'Anti-Preferences': df.iloc[:, 6]\n",
    "    })\n",
    "    \n",
    "    # Ensure anti-preferences are respected\n",
    "    for cluster in result_df['Cluster'].unique():\n",
    "        members = result_df[result_df['Cluster'] == cluster]\n",
    "        for _, row in members.iterrows():\n",
    "            anti_ids = process_anti_preferences(row['Anti-Preferences'], df['ID'].values)\n",
    "            conflict_members = members[members['ID'].isin(anti_ids)]\n",
    "            if not conflict_members.empty:\n",
    "                print(f\"Adjusting cluster {cluster} due to conflicts between {row['ID']} and {conflict_members['ID'].tolist()}\")\n",
    "                result_df.loc[result_df['ID'].isin(conflict_members['ID']), 'Cluster'] = -1\n",
    "    \n",
    "    return result_df.sort_values('Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_clusters(result_df):\n",
    "    \"\"\"Visualize clusters using a scatter plot.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x=result_df['Domains'].apply(lambda x: hash(x) % 100),  # Simplified domain hash\n",
    "        y=result_df['Cluster'],\n",
    "        hue=result_df['Cluster'],\n",
    "        palette='viridis',\n",
    "        s=100\n",
    "    )\n",
    "    plt.title(\"Student Clusters\")\n",
    "    plt.xlabel(\"Domains of Interest (Hashed)\")\n",
    "    plt.ylabel(\"Cluster\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    result = process_students('new_data.xlsx')\n",
    "    \n",
    "    # Save results\n",
    "    result.to_csv('student_clusters.csv', index=False)\n",
    "    \n",
    "    # Print cluster summary\n",
    "    print(\"\\nCluster Summary:\")\n",
    "    print(result.groupby('Cluster').agg(\n",
    "        Students=('ID', lambda x: x.tolist()),\n",
    "        Common_Domains=('Domains', lambda x: pd.Series(x).value_counts().index[0])\n",
    "    ).reset_index())\n",
    "    \n",
    "    # Visualize clusters\n",
    "    visualize_clusters(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
